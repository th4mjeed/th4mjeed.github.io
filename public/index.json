[{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by openâ€‘source software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemdâ€‘resolved by default, which conflicts with Sambaâ€™s internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Sambaâ€™s internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the builtâ€‘in Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domainâ€‘joined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings â†’ Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) â†’ type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windowsâ€‘compatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by openâ€‘source software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"}]
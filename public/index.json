[{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":"# Running IBM LSF Community Edition at Home ## Notes from a Painful Lab I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the *real* mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster. So I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing. This post is not a clean guide. It‚Äôs more like notes from what actually happened. --- ## Chapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not. But that‚Äôs exactly why I wanted to learn it. LSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is *very different* from Slurm. In Slurm, a login node is mostly just a machine that can talk to `slurmctld`. In LSF, **every host must be known, typed, and classified**. Even submit-only machines. That difference alone is worth learning. --- ## Chapter 2: The Cluster Layout I Used I kept the setup intentionally small. - `lsf-master` Runs LIM, mbatchd, acts as master candidate. - `lsf-node1` Execution node. - `fedora` My Fedora workstation, acting as a login / submission host. All machines could resolve each other via `/etc/hosts`. No DNS magic. I used LSF Community Edition binaries on Rocky Linux for the cluster nodes, Fedora for the login host. --- ## Chapter 3: Installing LSF (What Actually Matters) The installer itself is not the hard part. What matters is *where* LSF ends up and what gets configured. In my case, binaries landed under: /usr/local/lsf/10.1/linux3.10-glibc2.17-x86_64\nAnd configs lived in: /usr/local/lsf/conf\nThis distinction becomes important later. After installation, LSF was technically ‚Äúup‚Äù, but that doesn‚Äôt mean usable. --- ## Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing `profile.lsf` was enough. It is not. LSF does not work like that. When I ran: bhosts\non my Fedora machine, I kept getting: Failed in an LSF library call: LIM is down; try later\nEven though: - I could ping the master - LIM ports were reachable - SSH worked The problem wasn‚Äôt networking. The problem was that **LSF didn‚Äôt know my Fedora machine existed**. --- ## Chapter 5: `lsf.cluster` Is Not Optional This was the first major ‚Äúoh‚Äù moment. If a machine is going to submit jobs, it **must** appear in: /usr/local/lsf/conf/lsf.cluster.\nEven if: - it never runs jobs - it never runs RES - it is just a login box My `Host` section eventually looked like this: Begin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host\nThat `server 0` is important. Fedora is not an execution host. After this, errors changed, which is how you know you‚Äôre progressing. --- ## Chapter 6: `lsf.shared` ‚Äî The File Everyone Forgets Even after adding Fedora to the cluster file, I still got: Cannot find restarted or newly submitted job\u0026rsquo;s submission host and host type\nThis one took time to understand. LSF needs to know **what kind of machine** a host is: - architecture - OS type That mapping does **not** live in `lsf.cluster`. It lives in: /usr/local/lsf/conf/lsf.shared\nI had to explicitly define: - `HostType` (LINUX) - `HostModel` (X86_64) - and map Fedora to them Example: Begin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host\nOnly after this did `bhosts` stop rejecting Fedora. This is very different from Slurm. --- ## Chapter 7: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried: bsub -Is -XF xterm\nAnd hit a whole new class of errors: X11 connection rejected because of wrong authentication\nAt this point, LSF was fine. The cluster was fine. This was **pure SSH + X11 pain**. LSF uses: - SSH - X11 forwarding - `xauth` If any of these are missing on *any* hop, interactive jobs break in very confusing ways. --- ## Chapter 8: What Was Actually Missing The main issues were: - `xauth` not installed on all nodes - X11 forwarding not consistently enabled - SSH key-based auth not fully set up Installing `xauth` everywhere and making sure: X11Forwarding yes X11UseLocalhost no\nwas set on **all** machines finally fixed it. Only after this did: bsub -Is -XF -m lsf-master xterm\nactually open a window. --- ## Chapter 9: Why This Was Worth It This setup taught me things Slurm never forced me to learn: - LSF‚Äôs strict host classification model - The difference between submission, execution, and authentication - Why EDA environments lock things down so hard - Why interactive queues behave strangely compared to batch queues LSF feels old, but it‚Äôs old in a very deliberate way. --- ## Closing Thoughts This was not a smooth experience. I broke things, misread config files, and misunderstood errors more times than I can count. But I now understand **why LSF behaves the way it does**, and that was the whole point. If you‚Äôre coming from Slurm, LSF will feel hostile at first. But once it clicks, it starts to make sense. ","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gh\"\u003e# Running IBM LSF Community Edition at Home  \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gh\"\u003e\u003c/span\u003e\u003cspan class=\"gu\"\u003e## Notes from a Painful Lab\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cspan class=\"ge\"\u003e*real*\u003c/span\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e---\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e## Chapter 1: Why LSF and Not Slurm\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eI already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eBut that‚Äôs exactly why I wanted to learn it.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is \u003cspan class=\"ge\"\u003e*very different*\u003c/span\u003e from Slurm.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIn Slurm, a login node is mostly just a machine that can talk to \u003cspan class=\"sb\"\u003e`slurmctld`\u003c/span\u003e.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIn LSF, \u003cspan class=\"gs\"\u003e**every host must be known, typed, and classified**\u003c/span\u003e. Even submit-only machines.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eThat difference alone is worth learning.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e---\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e## Chapter 2: The Cluster Layout I Used\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eI kept the setup intentionally small.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003e-\u003c/span\u003e \u003cspan class=\"sb\"\u003e`lsf-master`\u003c/span\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Runs LIM, mbatchd, acts as master candidate.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003e-\u003c/span\u003e \u003cspan class=\"sb\"\u003e`lsf-node1`\u003c/span\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  Execution node.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003e-\u003c/span\u003e \u003cspan class=\"sb\"\u003e`fedora`\u003c/span\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  My Fedora workstation, acting as a login / submission host.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eAll machines could resolve each other via \u003cspan class=\"sb\"\u003e`/etc/hosts`\u003c/span\u003e. No DNS magic.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eI used LSF Community Edition binaries on Rocky Linux for the cluster nodes, Fedora for the login host.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e---\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e## Chapter 3: Installing LSF (What Actually Matters)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"gu\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eThe installer itself is not the hard part. What matters is \u003cspan class=\"ge\"\u003e*where*\u003c/span\u003e LSF ends up and what gets configured.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIn my case, binaries landed under:\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e/usr/local/lsf/10.1/linux3.10-glibc2.17-x86_64\u003c/p\u003e","title":""},{"content":" Notes from a Painful Lab I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nChapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"notes-from-a-painful-lab\"\u003eNotes from a Painful Lab\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nChapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nChapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up The Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting Up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up The Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the Ibm Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up The Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up The Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up The Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/ibm-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"},{"content":" Introduction I wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the real mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\nSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\nThis post is not a clean guide. It‚Äôs more like notes from what actually happened.\nChapter 1: Why LSF and Not Slurm I already have experience with Slurm, and I even wrote about using Slurm as a compilation farm. Slurm is straightforward. LSF is not.\nBut that‚Äôs exactly why I wanted to learn it.\nLSF still shows up in EDA, semiconductor companies, and older HPC environments. And the way it thinks about hosts, submission, and execution is very different from Slurm.\nIn Slurm, a login node is mostly just a machine that can talk to slurmctld.\nIn LSF, every host must be known, typed, and classified. Even submit-only machines.\nThat difference alone is worth learning.\nChapter 2: The Cluster Layout I Used I kept the setup intentionally small.\nlsf-master\nRuns LIM, mbatchd, acts as master candidate.\nlsf-node1\nExecution node.\nfedora\nMy Fedora workstation, acting as a login / submission host.\nAll machines could resolve each other via /etc/hosts. No DNS magic.\nI used Rocky Linux for lsf-master and lsf-node1, and Fedora for my submission machine.\nChapter 3: Getting LSF This part deserves its own chapter because this is where things already got weird.\nThere is no obvious ‚Äúdownload LSF‚Äù button.\nI actually found the IBM Spectrum LSF Community Edition through a Reddit post. That post linked to IBM‚Äôs site, which then required me to:\nCreate an IBM account Log in Accept licensing terms Navigate through IBM‚Äôs download portal Only after all that was I able to download the tarball.\nWhat I ended up with was something like:\nlsfsce10.2.0.15-x86_64.tar.Z This already tells you something important:\nIBM ships prebuilt binaries They are tied to a specific glibc and kernel baseline That becomes important later when Fedora starts complaining.\nChapter 3.1: Extracting the Tarball On the master node (lsf-master), I extracted it under /tmp first:\ntar -xvf lsfsce10.2.0.15-x86_64.tar.Z cd lsfsce10.2.0.15-x86_64 Inside, you don‚Äôt get a fancy installer. You get scripts.\nThe real entry point is:\n./lsfinstall But do not run it blindly.\nChapter 3.2: install.config Is the Real Installer LSF installation is driven almost entirely by a file called:\ninstall.config If you mess this up, the installer will still run, but you‚Äôll regret it later.\nThis is roughly what I used (simplified to the important parts):\nLSF_TOP=\u0026#34;/usr/local/lsf\u0026#34; LSF_ADMINS=\u0026#34;edauser\u0026#34; LSF_CLUSTER_NAME=\u0026#34;lsfcluster\u0026#34; LSF_MASTER_LIST=\u0026#34;lsf-master\u0026#34; LSF_SERVER_HOSTS=\u0026#34;lsf-master lsf-node1\u0026#34; ENABLE_EGO=\u0026#34;Y\u0026#34; EGO_DAEMON_CONTROL=\u0026#34;Y\u0026#34; LSF_LOCAL_RESOURCES=\u0026#34;[resource mg]\u0026#34; Things that matter here:\nLSF_TOP\nThis decides everything. Binaries, configs, logs. I kept it simple.\nLSF_ADMINS\nThis user must exist. I created edauser beforehand.\nLSF_CLUSTER_NAME\nThis name shows up everywhere later. Pick once, don‚Äôt change it.\nLSF_MASTER_LIST\nThis is not optional. If this is wrong, LIM will never behave.\nLSF_SERVER_HOSTS\nThese are execution-capable hosts. Submission-only hosts do NOT go here.\nAt this stage, Fedora was not included anywhere.\nChapter 3.3: Running the Installer Only after editing install.config did I run:\n./lsfinstall -f install.config The installer:\ncreates /usr/local/lsf drops binaries under versioned directories writes configs under /usr/local/lsf/conf When it finished, I had:\n/usr/local/lsf/10.1/ /usr/local/lsf/conf/ /usr/local/lsf/work/ At this point, LSF was technically ‚Äúinstalled‚Äù.\nThat does not mean usable.\nChapter 3.4: Environment Setup LSF does nothing unless you source its environment and start the daemons.\nOn the master and nodes:\nsource /usr/local/lsf/conf/profile.lsf Without this:\nbhosts won‚Äôt work lsid won‚Äôt work errors will be extremely misleading I added this to the admin user‚Äôs shell profile later, but initially I sourced it manually.\nTo start the daemons I used the commad:\nlsf_daemons start Chapter 4: The First Big Reality Check ‚Äî Hosts Must Be Known One mistake I made early was assuming that copying the tarball to another machine and sourcing profile.lsf was enough.\nIt is not.\nLSF does not work like that.\nWhen I ran:\nbhosts on my Fedora machine, I kept getting:\nFailed in an LSF library call: LIM is down; try later Even though:\nI could ping the master LIM ports were reachable SSH worked The problem wasn‚Äôt networking.\nThe problem was that LSF didn‚Äôt know my Fedora machine existed.\nChapter 5: Teaching LSF That Fedora Exists (and What It Is) This was the first major ‚Äúoh‚Äù moment.\nI kept assuming that since Fedora was only a submit host, LSF wouldn‚Äôt really care about it. That assumption was wrong.\nIf a machine is going to submit jobs, it must appear in the cluster definition file:\n/usr/local/lsf/conf/lsf.cluster.\u0026lt;clustername\u0026gt; Even if:\nit never runs jobs it never runs RES it is just a login box LSF is very literal about this.\nMy Host section eventually looked like this:\nBegin Host HOSTNAME model type server RESOURCES lsf-master ! ! 1 (mg) lsf-node1 ! ! 1 () fedora ! ! 0 () End Host That server 0 is important. Fedora is not an execution host. It only submits jobs.\nOnce I added Fedora here and ran:\nlsadmin reconfig the errors changed. They didn‚Äôt disappear, but they changed, which is usually how you know you‚Äôre moving forward with LSF.\nAt this point, I thought I was done.\nI wasn‚Äôt.\nEven after Fedora was listed in lsf.cluster, I kept getting this error:\nCannot find restarted or newly submitted job\u0026#39;s submission host and host type This one took time to understand.\nLSF doesn‚Äôt just want to know that a host exists.\nIt also wants to know what kind of host it is:\narchitecture operating system That information does not live in lsf.cluster.\nIt lives in:\n/usr/local/lsf/conf/lsf.shared This is the file almost everyone forgets.\nI had to explicitly define:\nthe host model (X86_64) the host type (LINUX) and map every host, including Fedora This is what finally fixed it:\nBegin Host HOSTNAME model type lsf-master X86_64 LINUX lsf-node1 X86_64 LINUX fedora X86_64 LINUX End Host After saving this file, I ran:\nlsadmin reconfig Only after this did bhosts stop rejecting Fedora and job submission start behaving like it should.\nThis was a big lesson for me:\nLSF will happily run with half the information missing, but it will fail in ways that make it feel like something much deeper is broken.\nChapter 6: Interactive Jobs Work‚Ä¶ Until X11 Shows Up Once submission worked, I tried:\nbsub -Is -XF xterm And hit a whole new class of errors:\nX11 connection rejected because of wrong authentication At this point, LSF was fine.\nThe cluster was fine.\nThis was pure SSH + X11 pain.\nWhat Was Actually Missing\nThe main issues were:\nxauth not installed on all nodes X11 forwarding not consistently enabled SSH key-based auth not fully set up Installing xauth everywhere and making sure:\nX11Forwarding yes X11UseLocalhost no was set on all machines finally fixed it.\nOnly after this did:\nbsub -Is -XF -m lsf-master xterm actually open a window.\nClosing Thoughts At this point, the cluster was stable enough for what I wanted to test.\nI initially planned to go one step further and add a proper license server using FlexLM, similar to how it‚Äôs done in real EDA environments. The idea was to tie license availability into scheduling and see how LSF behaves under those constraints.\nBut I decided to stop here.\nThe goal of this setup was to understand:\nhow LSF components talk to each other how submit hosts differ from execution hosts and what minimum configuration is actually required for things to work That part was done.\nLicense servers can come later as a separate iteration.\n","permalink":"http://localhost:1313/posts/setting-up-community-edition-lsf/","summary":"\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI wanted to understand how LSF actually works, not just at a surface ‚Äúsubmit job, job runs‚Äù level, but the \u003cem\u003ereal\u003c/em\u003e mechanics. Most places that use LSF don‚Äôt document it well, and when they do, it‚Äôs written assuming you already run a production cluster.\u003c/p\u003e\n\u003cp\u003eSo I decided to build a small LSF Community Edition cluster at home and treat it like an EDA-style environment: master node, execution nodes, login-style submission, interactive jobs, X11, the whole thing.\u003c/p\u003e","title":"Setting up the Community Edition of IBM LSF (Load Sharing Facility)"},{"content":" In this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\nThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\nWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\nPreparing the Domain Controller System We begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\nSet the hostname:\nhostnamectl set-hostname dc1.internal.lan Add the server address to /etc/hosts so DNS lookups resolve locally during installation:\n10.10.40.90 dc1.internal.lan dc1 Next, update packages and install the Samba DC components along with Kerberos and DNS utilities:\nsudo dnf update -y sudo dnf install -y samba samba-dc samba-dns samba-winbind krb5-workstation bind-utils Fedora uses systemd‚Äëresolved by default, which conflicts with Samba‚Äôs internal DNS. We disable it and ensure Samba will handle DNS:\nsudo systemctl disable --now systemd-resolved sudo rm -f /etc/resolv.conf Then create a new resolv.conf pointing DNS to the AD server itself:\necho \u0026#34;nameserver 127.0.0.1\u0026#34; | sudo tee /etc/resolv.conf echo \u0026#34;search internal.lan\u0026#34; | sudo tee -a /etc/resolv.conf At this point, the server is ready for domain provisioning.\nProvisioning the Active Directory Domain Samba includes a provisioning tool that creates the directory structure, Kerberos configuration, and internal DNS zones.\nRun provisioning interactively:\nsudo samba-tool domain provision --use-rfc2307 --interactive During setup:\nRealm can be set to INTERNAL.LAN Domain name can be INTERNAL Server role must be dc (domain controller) Select SAMBA_INTERNAL DNS backend When provisioning completes, start the Samba service:\nsudo systemctl enable --now samba To verify that DNS is functioning, check SRV records for Kerberos and LDAP:\nhost -t SRV _kerberos._udp.internal.lan host -t SRV _ldap._tcp.internal.lan host -t A dc1.internal.lan Confirm that Samba‚Äôs internal services are running properly:\nsudo samba-tool processes Verifying Kerberos Authentication Kerberos is central to Active Directory authentication. We test it using the built‚Äëin Administrator account.\nkinit administrator@INTERNAL.LAN klist If everything is configured correctly, a Kerberos ticket will be displayed.\nCreating Users in Active Directory To add a test user:\nsamba-tool user create employee1 This user can later be used to log in to domain‚Äëjoined machines.\nJoining a Fedora Client to the Domain On the Fedora client, install the tools required for AD domain membership:\nsudo dnf install -y realmd sssd adcli oddjob oddjob-mkhomedir samba-winbind-clients Discover the domain:\nrealm discover internal.lan Join the domain using administrative credentials:\nsudo realm join internal.lan -U administrator Enable automatic home directory creation for domain users:\nsudo pam-auth-update --enable mkhomedir Now, domain accounts can be used to log in through the graphical login screen using:\nINTERNAL\\employee1 Joining a Windows 11 Client to the Domain Windows 11 systems can be joined to the Samba Active Directory domain, enabling centralized authentication just like in a Microsoft AD environment.\nConfigure DNS on Windows 11 Before joining the domain, ensure the Windows machine uses the domain controller for DNS resolution:\nOpen Settings ‚Üí Network \u0026amp; Internet\nSelect the active network adapter\nChoose Edit DNS settings\nSet it to Manual and configure:\nPreferred DNS: 10.10.40.90 Test DNS resolution using Command Prompt:\nping dc1.internal.lan Join Windows to the Domain Open Run (Win + R) ‚Üí type: sysdm.cpl Go to the Computer Name tab Click Change and select Domain Enter: internal.lan Provide domain credentials when prompted (such as INTERNAL\\administrator) Reboot the system Validate Domain Login After reboot, log in using:\nINTERNAL\\employee1 Then verify domain membership:\nsysteminfo | findstr /i domain Final Notes With Samba successfully serving as an Active Directory Domain Controller, Linux systems can now authenticate against the domain and Kerberos security is fully operational.\nFuture enhancements may include:\nInstalling RSAT on Windows 11 for easy management of User and Groups Configuring Group Policy through Samba tools Securing DNS and directory traffic with TLS certificates Adding domain file services with Windows‚Äëcompatible permissions Integrating an AD Certificate Services alternative for Kerberos PKINIT ","permalink":"http://localhost:1313/posts/setting-up-samba-ad/","summary":"\u003chr\u003e\n\u003cp\u003eIn this guide, we walk through the process of setting up a Samba Active Directory Domain Controller. The goal is to create a fully functional AD environment with the ability to join Windows and Linux clients to the domain.\u003c/p\u003e\n\u003cp\u003eThis setup mirrors how Microsoft Active Directory works, but fully powered by open‚Äësource software.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using Fedora Linux to configure Samba AD, but you can use any Linux\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"preparing-the-domain-controller-system\"\u003ePreparing the Domain Controller System\u003c/h2\u003e\n\u003cp\u003eWe begin by configuring Fedora 43 to act as a domain controller. The hostname must reflect the fully qualified domain name of the AD server.\u003c/p\u003e","title":"Setting Up Samba as an Active Directory Domain Controller on Linux"},{"content":" Note: üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; -y \u0026amp;\u0026amp; sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\u0026#34;/mnt/slurm_share/linux-6.8.9\u0026#34; BUILD_OUTPUT_DIR=\u0026#34;/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\u0026#34; mkdir -p \u0026#34;$BUILD_OUTPUT_DIR\u0026#34; cd \u0026#34;$KERNEL_SOURCE_PATH\u0026#34; NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\u0026#34;${NUM_MAKE_JOBS}\u0026#34; ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \u0026#34;$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\u0026#34; \u0026#34;$BUILD_OUTPUT_DIR/\u0026#34; else echo \u0026#34;Kernel compilation failed.\u0026#34; fi ","permalink":"http://localhost:1313/posts/slurm-as-a-compilation-farm/","summary":"\u003chr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e üì¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noob‚Äôs perspective diving into it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThis guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\u003c/p\u003e","title":"SLURM as a Compilation Farm"}]
<!doctype html><html lang=en dir=auto data-theme=dark><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SLURM as a Compilation Farm | th4mjeed</title><meta name=keywords content><meta name=description content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.

Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><meta name=author content="Thamjeed"><link rel=canonical href=http://localhost:1313/posts/slurm-as-a-compilation-farm/><link crossorigin=anonymous href=/assets/css/stylesheet.fde665597d1ffebd00cc9ba2cd6aed5b8855880ddf943c59640dcc99d68f5505.css integrity="sha256-/eZlWX0f/r0AzJuizWrtW4hViA3flDxZZA3MmdaPVQU=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/slurm-as-a-compilation-farm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/slurm-as-a-compilation-farm/"><meta property="og:site_name" content="th4mjeed"><meta property="og:title" content="SLURM as a Compilation Farm"><meta property="og:description" content=" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-19T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="SLURM as a Compilation Farm"><meta name=twitter:description content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.

Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"SLURM as a Compilation Farm","item":"http://localhost:1313/posts/slurm-as-a-compilation-farm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SLURM as a Compilation Farm","name":"SLURM as a Compilation Farm","description":" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\n","keywords":[],"articleBody":" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \"Development Tools\" sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \"Development Tools\" -y \u0026\u0026 sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\"/mnt/slurm_share/linux-6.8.9\" BUILD_OUTPUT_DIR=\"/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\" mkdir -p \"$BUILD_OUTPUT_DIR\" cd \"$KERNEL_SOURCE_PATH\" NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\"${NUM_MAKE_JOBS}\" ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \"$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\" \"$BUILD_OUTPUT_DIR/\" else echo \"Kernel compilation failed.\" fi ","wordCount":"766","inLanguage":"en","datePublished":"2025-11-19T00:00:00Z","dateModified":"2025-11-19T00:00:00Z","author":{"@type":"Person","name":"Thamjeed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/slurm-as-a-compilation-farm/"},"publisher":{"@type":"Organization","name":"th4mjeed","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="th4mjeed (Alt + H)">th4mjeed</a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/posts title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">SLURM as a Compilation Farm</h1><div class=post-meta><span title='2025-11-19 00:00:00 +0000 UTC'>November 19, 2025</span>&nbsp;Â·&nbsp;<span>4 min</span>&nbsp;Â·&nbsp;<span>Thamjeed</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction:>Introduction:</a></li><li><a href=#lab-infrastructure aria-label="Lab Infrastructure:">Lab Infrastructure:</a></li><li><a href=#chapter-1-the-installation aria-label="Chapter 1: The installation:">Chapter 1: The installation:</a></li><li><a href=#chapter-2-the-configuration aria-label="Chapter 2: The Configuration:">Chapter 2: The Configuration:</a></li><li><a href=#chapter-3-testing-and-introduction-to-the-commands aria-label="Chapter 3: Testing and Introduction to the commands:">Chapter 3: Testing and Introduction to the commands:</a></li><li><a href=#chapter-4-setting-up-the-nfs-storage aria-label="Chapter 4: Setting up the NFS storage.">Chapter 4: Setting up the NFS storage.</a></li><li><a href=#chapter-5-setting-up-the-compilebuild-environment aria-label="Chapter 5: Setting up the Compile/Build Environment">Chapter 5: Setting up the Compile/Build Environment</a></li></ul></div></details></div><div class=post-content><hr><blockquote><p><strong>Note:</strong> ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.</p></blockquote><h2 id=introduction><strong>Introduction:</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.</p><h2 id=lab-infrastructure><strong>Lab Infrastructure:</strong><a hidden class=anchor aria-hidden=true href=#lab-infrastructure>#</a></h2><p>The following are all on VMware ESXI</p><ol><li><p><strong>Master:</strong></p><ul><li>CPUs 4</li><li>Memory 4 GB</li><li>Hard disk 20 GB</li><li>Hostname: master</li></ul></li><li><p><strong>Node 1:</strong></p><ul><li>CPUs 4</li><li>Memory 4 GB</li><li>Hard disk 40 GB</li><li>Hostname: node1</li></ul></li><li><p><strong>Node 2:</strong></p><ul><li>CPUs 8</li><li>Memory 8 GB</li><li>Hard disk 40 GB</li><li>Hostname: node2</li></ul></li><li><p><strong>Network File Storage</strong></p><ul><li>Since compiling creates dozens of files, at least 30 GB is required for a successful compilation.</li><li>Used the existing testing server assigned to me.</li><li>NFS share path located in <strong>/mnt/slrum_share</strong></li></ul></li></ol><p>Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the <code>/etc/hosts</code> file.</p><h2 id=chapter-1-the-installation><strong>Chapter 1: The installation:</strong><a hidden class=anchor aria-hidden=true href=#chapter-1-the-installation>#</a></h2><ol><li><p><strong>Install and configure dependencies</strong></p><p>Installation of slurm requires EPEL repo to be installed across all instances, install and enable it via:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>dnf config-manager --set-enabled crb
</span></span><span class=line><span class=cl>dnf install epel-release
</span></span><span class=line><span class=cl>sudo dnf groupinstall <span class=s2>&#34;Development Tools&#34;</span>
</span></span><span class=line><span class=cl>sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel</span></span></code></pre></div><p>MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo useradd munge
</span></span><span class=line><span class=cl>sudo mkdir -p /etc/munge /var/log/munge /var/run/munge
</span></span><span class=line><span class=cl>sudo chown munge:munge /usr/local/var/run/munge
</span></span><span class=line><span class=cl>sudo chmod <span class=m>0755</span> /usr/local/var/run/munge</span></span></code></pre></div><p>On Master:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo /usr/sbin/create-munge-key
</span></span><span class=line><span class=cl>sudo chown munge:munge /etc/munge/munge.key
</span></span><span class=line><span class=cl>sudo chmod <span class=m>0400</span> /etc/munge/munge.key</span></span></code></pre></div><p>Copy the key to both nodes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scp /etc/munge/munge.key root@node1:/etc/munge/
</span></span><span class=line><span class=cl>scp /etc/munge/munge.key root@node2:/etc/munge/</span></span></code></pre></div><p>Start and enable the service:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo systemctl <span class=nb>enable</span> --now munge</span></span></code></pre></div></li><li><p><strong>Installation of SLURM</strong></p><p>Slurm is available in the EPEL repo. Install on all 3 instances:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi</span></span></code></pre></div><p>If by any chance packages are not available, download tar file from <a href=https://www.schedmd.com/download-slurm/>SchedMD Downloads</a>, extract, compile, and install using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>make -j<span class=k>$(</span>nproc<span class=k>)</span>
</span></span><span class=line><span class=cl>sudo make install</span></span></code></pre></div></li></ol><h2 id=chapter-2-the-configuration><strong>Chapter 2: The Configuration:</strong><a hidden class=anchor aria-hidden=true href=#chapter-2-the-configuration>#</a></h2><ol><li><p><strong>Slurm configuration</strong></p><p>On all 3 instances:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo useradd slurm
</span></span><span class=line><span class=cl>sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
</span></span><span class=line><span class=cl>sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm</span></span></code></pre></div><p>Edit the configuration on master:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo nano /etc/slurm/slurm.conf</span></span></code></pre></div><p>Ensure the following key lines are present and correctly configured:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>ClusterName=debug
</span></span><span class=line><span class=cl>SlurmUser=slurm
</span></span><span class=line><span class=cl>ControlMachine=slurm-master
</span></span><span class=line><span class=cl>SlurmctldPort=6817
</span></span><span class=line><span class=cl>SlurmdPort=6818
</span></span><span class=line><span class=cl>AuthType=auth/munge
</span></span><span class=line><span class=cl>StateSaveLocation=/var/spool/slurmctld
</span></span><span class=line><span class=cl>SlurmdSpoolDir=/var/spool/slurmd
</span></span><span class=line><span class=cl>SwitchType=switch/none
</span></span><span class=line><span class=cl>MpiDefault=none
</span></span><span class=line><span class=cl>SlurmctldPidFile=/var/run/slurmctld.pid
</span></span><span class=line><span class=cl>SlurmdPidFile=/var/run/slurmd.pid
</span></span><span class=line><span class=cl>ProctrackType=proctrack/pgid
</span></span><span class=line><span class=cl>ReturnToService=1
</span></span><span class=line><span class=cl>SchedulerType=sched/backfill
</span></span><span class=line><span class=cl>SlurmctldTimeout=300
</span></span><span class=line><span class=cl>SlurmdTimeout=30
</span></span><span class=line><span class=cl>NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN
</span></span><span class=line><span class=cl>NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN
</span></span><span class=line><span class=cl>PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP</span></span></code></pre></div><p>Copy configuration to nodes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf
</span></span><span class=line><span class=cl>scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf</span></span></code></pre></div><p>Start and enable services:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo systemctl <span class=nb>enable</span> --now slurmctld
</span></span><span class=line><span class=cl>sudo systemctl <span class=nb>enable</span> --now slurmd</span></span></code></pre></div></li><li><p><strong>Firewall Configuration:</strong></p><p>Open required ports:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo firewall-cmd --permanent --add-port<span class=o>=</span>6817/tcp
</span></span><span class=line><span class=cl>sudo firewall-cmd --permanent --add-port<span class=o>=</span>6818/tcp
</span></span><span class=line><span class=cl>sudo firewall-cmd --permanent --add-port<span class=o>=</span>6819/tcp
</span></span><span class=line><span class=cl>sudo firewall-cmd --reload</span></span></code></pre></div></li></ol><h2 id=chapter-3-testing-and-introduction-to-the-commands><strong>Chapter 3: Testing and Introduction to the commands:</strong><a hidden class=anchor aria-hidden=true href=#chapter-3-testing-and-introduction-to-the-commands>#</a></h2><p><strong>(While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)</strong></p><ol><li><p><code>sinfo</code>:</p><p>Displays node and partition information:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sinfo</span></span></code></pre></div></li><li><p><code>srun</code>:</p><p>Runs commands interactively on compute nodes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>srun -N2 -n2 nproc</span></span></code></pre></div></li><li><p><code>sbatch</code>:</p><p>Submits a job script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sbatch testjob.sh</span></span></code></pre></div></li><li><p><code>squeue</code>:</p><p>Displays details of currently running jobs:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>squeue</span></span></code></pre></div></li><li><p><code>scancel</code>:</p><p>Cancels a submitted job:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scancel <span class=m>1</span></span></span></code></pre></div></li><li><p><code>scontrol</code>:</p><p>Displays detailed job and node information:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scontrol show job <span class=m>1</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>scontrol show partition</span></span></code></pre></div></li></ol><h2 id=chapter-4-setting-up-the-nfs-storage><strong>Chapter 4: Setting up the NFS storage.</strong><a hidden class=anchor aria-hidden=true href=#chapter-4-setting-up-the-nfs-storage>#</a></h2><p>It is a good idea to have shared storage for SLURM. Install <code>nfs-utils</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo dnf install nfs-utils</span></span></code></pre></div><p><strong>On the NFS server:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mkdir /srv/slurm_share
</span></span><span class=line><span class=cl>nano /etc/exports</span></span></code></pre></div><p>Add the following line:</p><pre tabindex=0><code>/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)
</code></pre><p>Open necessary ports:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>firewall-cmd --permanent --add-service<span class=o>=</span>rpc-bind
</span></span><span class=line><span class=cl>firewall-cmd --permanent --add-port<span class=o>={</span>5555/tcp,5555/udp,6666/tcp,6666/udp<span class=o>}</span>
</span></span><span class=line><span class=cl>firewall-cmd --reload</span></span></code></pre></div><p>Export and enable the service:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>exportfs -v
</span></span><span class=line><span class=cl>systemctl <span class=nb>enable</span> --now nfs-server</span></span></code></pre></div><p><strong>On the master and compute nodes:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo mkdir /mnt/slurm_share</span></span></code></pre></div><p>Add the mount in <code>/etc/fstab</code>:</p><pre tabindex=0><code>10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0
</code></pre><p>Reboot machines and verify the share mounts properly.</p><h2 id=chapter-5-setting-up-the-compilebuild-environment><strong>Chapter 5: Setting up the Compile/Build Environment</strong><a hidden class=anchor aria-hidden=true href=#chapter-5-setting-up-the-compilebuild-environment>#</a></h2><p>Install kernel build dependencies:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>srun -n2 -N2 sudo dnf groupinstall <span class=s2>&#34;Development Tools&#34;</span> -y <span class=o>&amp;&amp;</span> sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y</span></span></code></pre></div><p>Download the Linux kernel source from <a href=https://kernel.org>kernel.org</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>wget <span class=o>[</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class=o>](</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class=o>)</span>
</span></span><span class=line><span class=cl>tar xvf linux-6.14.8.tar.xz</span></span></code></pre></div><p>Define architecture-specific config:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>make defconfig</span></span></code></pre></div><p>Create <code>compile_kernel.sh</code> on the shared directory:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>#SBATCH --job-name=kernel_build</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --output=kernel_build_%j.out</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --error=kernel_build_%j.err</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --time=03:00:00</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --nodes=1</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --cpus-per-task=8</span>
</span></span><span class=line><span class=cl><span class=c1>#SBATCH --mem=8G</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>KERNEL_SOURCE_PATH</span><span class=o>=</span><span class=s2>&#34;/mnt/slurm_share/linux-6.8.9&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>BUILD_OUTPUT_DIR</span><span class=o>=</span><span class=s2>&#34;/mnt/slurm_share/kernel_builds/</span><span class=si>${</span><span class=nv>SLURM_JOB_ID</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>mkdir -p <span class=s2>&#34;</span><span class=nv>$BUILD_OUTPUT_DIR</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> <span class=s2>&#34;</span><span class=nv>$KERNEL_SOURCE_PATH</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>NUM_MAKE_JOBS</span><span class=o>=</span><span class=si>${</span><span class=nv>SLURM_CPUS_PER_TASK</span><span class=si>}</span>
</span></span><span class=line><span class=cl>make -j<span class=s2>&#34;</span><span class=si>${</span><span class=nv>NUM_MAKE_JOBS</span><span class=si>}</span><span class=s2>&#34;</span> <span class=nv>ARCH</span><span class=o>=</span>x86_64 Image modules dtbs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=o>[</span> <span class=nv>$?</span> -eq <span class=m>0</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>cp <span class=s2>&#34;</span><span class=nv>$KERNEL_SOURCE_PATH</span><span class=s2>/arch/x86/boot/bzImage&#34;</span> <span class=s2>&#34;</span><span class=nv>$BUILD_OUTPUT_DIR</span><span class=s2>/&#34;</span>
</span></span><span class=line><span class=cl><span class=k>else</span>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;Kernel compilation failed.&#34;</span>
</span></span><span class=line><span class=cl><span class=k>fi</span></span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/setting-up-samba-ad/><span class=title>Â« Prev</span><br><span>Setting Up Samba as an Active Directory Domain Controller on Linux</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on x" href="https://x.com/intent/tweet/?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&amp;title=SLURM%20as%20a%20Compilation%20Farm&amp;summary=SLURM%20as%20a%20Compilation%20Farm&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&title=SLURM%20as%20a%20Compilation%20Farm"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on whatsapp" href="https://api.whatsapp.com/send?text=SLURM%20as%20a%20Compilation%20Farm%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on telegram" href="https://telegram.me/share/url?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on ycombinator" href="https://news.ycombinator.com/submitlink?t=SLURM%20as%20a%20Compilation%20Farm&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>Â© Thamjeed</span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
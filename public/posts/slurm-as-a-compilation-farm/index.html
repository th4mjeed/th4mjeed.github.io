<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>SLURM as a Compilation Farm | th4mjeed</title>
<meta name="keywords" content="">
<meta name="description" content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.">
<meta name="author" content="Thamjeed">
<link rel="canonical" href="http://localhost:1313/posts/slurm-as-a-compilation-farm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.46d7a9b8bdcee445fce1387b698d10c4568e9c8c10d43bf905cd321bc58d4099.css" integrity="sha256-RtepuL3O5EX84Th7aY0QxFaOnIwQ1Dv5Bc0yG8WNQJk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/slurm-as-a-compilation-farm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/slurm-as-a-compilation-farm/">
  <meta property="og:site_name" content="th4mjeed">
  <meta property="og:title" content="SLURM as a Compilation Farm">
  <meta property="og:description" content=" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-06T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SLURM as a Compilation Farm">
<meta name="twitter:description" content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SLURM as a Compilation Farm",
      "item": "http://localhost:1313/posts/slurm-as-a-compilation-farm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SLURM as a Compilation Farm",
  "name": "SLURM as a Compilation Farm",
  "description": " Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\n",
  "keywords": [
    
  ],
  "articleBody": " Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \"Development Tools\" sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \"Development Tools\" -y \u0026\u0026 sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\"/mnt/slurm_share/linux-6.8.9\" BUILD_OUTPUT_DIR=\"/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\" mkdir -p \"$BUILD_OUTPUT_DIR\" cd \"$KERNEL_SOURCE_PATH\" NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\"${NUM_MAKE_JOBS}\" ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \"$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\" \"$BUILD_OUTPUT_DIR/\" else echo \"Kernel compilation failed.\" fi ",
  "wordCount" : "766",
  "inLanguage": "en",
  "datePublished": "2025-11-06T00:00:00Z",
  "dateModified": "2025-11-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thamjeed"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/slurm-as-a-compilation-farm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "th4mjeed",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="th4mjeed (Alt + H)">th4mjeed</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      SLURM as a Compilation Farm
    </h1>
    <div class="post-meta"><span title='2025-11-06 00:00:00 +0000 UTC'>November 6, 2025</span>&nbsp;Â·&nbsp;<span>4 min</span>&nbsp;Â·&nbsp;<span>Thamjeed</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction:">Introduction:</a></li>
                <li>
                    <a href="#lab-infrastructure" aria-label="Lab Infrastructure:">Lab Infrastructure:</a></li>
                <li>
                    <a href="#chapter-1-the-installation" aria-label="Chapter 1: The installation:">Chapter 1: The installation:</a></li>
                <li>
                    <a href="#chapter-2-the-configuration" aria-label="Chapter 2: The Configuration:">Chapter 2: The Configuration:</a></li>
                <li>
                    <a href="#chapter-3-testing-and-introduction-to-the-commands" aria-label="Chapter 3: Testing and Introduction to the commands:">Chapter 3: Testing and Introduction to the commands:</a></li>
                <li>
                    <a href="#chapter-4-setting-up-the-nfs-storage" aria-label="Chapter 4: Setting up the NFS storage.">Chapter 4: Setting up the NFS storage.</a></li>
                <li>
                    <a href="#chapter-5-setting-up-the-compilebuild-environment" aria-label="Chapter 5: Setting up the Compile/Build Environment">Chapter 5: Setting up the Compile/Build Environment</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><hr>
<blockquote>
<p><strong>Note:</strong> ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.</p></blockquote>
<h2 id="introduction"><strong>Introduction:</strong><a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.</p>
<h2 id="lab-infrastructure"><strong>Lab Infrastructure:</strong><a hidden class="anchor" aria-hidden="true" href="#lab-infrastructure">#</a></h2>
<p>The following are all on VMware ESXI</p>
<ol>
<li>
<p><strong>Master:</strong></p>
<ul>
<li>CPUs 4</li>
<li>Memory 4 GB</li>
<li>Hard disk 20 GB</li>
<li>Hostname: master</li>
</ul>
</li>
<li>
<p><strong>Node 1:</strong></p>
<ul>
<li>CPUs 4</li>
<li>Memory 4 GB</li>
<li>Hard disk 40 GB</li>
<li>Hostname: node1</li>
</ul>
</li>
<li>
<p><strong>Node 2:</strong></p>
<ul>
<li>CPUs 8</li>
<li>Memory 8 GB</li>
<li>Hard disk 40 GB</li>
<li>Hostname: node2</li>
</ul>
</li>
<li>
<p><strong>Network File Storage</strong></p>
<ul>
<li>Since compiling creates dozens of files, at least 30 GB is required for a successful compilation.</li>
<li>Used the existing testing server assigned to me.</li>
<li>NFS share path located in <strong>/mnt/slrum_share</strong></li>
</ul>
</li>
</ol>
<p>Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the <code>/etc/hosts</code> file.</p>
<h2 id="chapter-1-the-installation"><strong>Chapter 1: The installation:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-1-the-installation">#</a></h2>
<ol>
<li>
<p><strong>Install and configure dependencies</strong></p>
<p>Installation of slurm requires EPEL repo to be installed across all instances, install and enable it via:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">dnf config-manager --set-enabled crb
</span></span><span class="line"><span class="cl">dnf install epel-release
</span></span><span class="line"><span class="cl">sudo dnf groupinstall <span class="s2">&#34;Development Tools&#34;</span>
</span></span><span class="line"><span class="cl">sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel</span></span></code></pre></div>
<p>MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo useradd munge
</span></span><span class="line"><span class="cl">sudo mkdir -p /etc/munge /var/log/munge /var/run/munge
</span></span><span class="line"><span class="cl">sudo chown munge:munge /usr/local/var/run/munge
</span></span><span class="line"><span class="cl">sudo chmod <span class="m">0755</span> /usr/local/var/run/munge</span></span></code></pre></div>
<p>On Master:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo /usr/sbin/create-munge-key
</span></span><span class="line"><span class="cl">sudo chown munge:munge /etc/munge/munge.key
</span></span><span class="line"><span class="cl">sudo chmod <span class="m">0400</span> /etc/munge/munge.key</span></span></code></pre></div>
<p>Copy the key to both nodes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scp /etc/munge/munge.key root@node1:/etc/munge/
</span></span><span class="line"><span class="cl">scp /etc/munge/munge.key root@node2:/etc/munge/</span></span></code></pre></div>
<p>Start and enable the service:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now munge</span></span></code></pre></div>
</li>
<li>
<p><strong>Installation of SLURM</strong></p>
<p>Slurm is available in the EPEL repo. Install on all 3 instances:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi</span></span></code></pre></div>
<p>If by any chance packages are not available, download tar file from <a href="https://www.schedmd.com/download-slurm/">SchedMD Downloads</a>, extract, compile, and install using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make -j<span class="k">$(</span>nproc<span class="k">)</span>
</span></span><span class="line"><span class="cl">sudo make install</span></span></code></pre></div>
</li>
</ol>
<h2 id="chapter-2-the-configuration"><strong>Chapter 2: The Configuration:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-2-the-configuration">#</a></h2>
<ol>
<li>
<p><strong>Slurm configuration</strong></p>
<p>On all 3 instances:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo useradd slurm
</span></span><span class="line"><span class="cl">sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
</span></span><span class="line"><span class="cl">sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm</span></span></code></pre></div>
<p>Edit the configuration on master:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo nano /etc/slurm/slurm.conf</span></span></code></pre></div>
<p>Ensure the following key lines are present and correctly configured:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">ClusterName=debug
</span></span><span class="line"><span class="cl">SlurmUser=slurm
</span></span><span class="line"><span class="cl">ControlMachine=slurm-master
</span></span><span class="line"><span class="cl">SlurmctldPort=6817
</span></span><span class="line"><span class="cl">SlurmdPort=6818
</span></span><span class="line"><span class="cl">AuthType=auth/munge
</span></span><span class="line"><span class="cl">StateSaveLocation=/var/spool/slurmctld
</span></span><span class="line"><span class="cl">SlurmdSpoolDir=/var/spool/slurmd
</span></span><span class="line"><span class="cl">SwitchType=switch/none
</span></span><span class="line"><span class="cl">MpiDefault=none
</span></span><span class="line"><span class="cl">SlurmctldPidFile=/var/run/slurmctld.pid
</span></span><span class="line"><span class="cl">SlurmdPidFile=/var/run/slurmd.pid
</span></span><span class="line"><span class="cl">ProctrackType=proctrack/pgid
</span></span><span class="line"><span class="cl">ReturnToService=1
</span></span><span class="line"><span class="cl">SchedulerType=sched/backfill
</span></span><span class="line"><span class="cl">SlurmctldTimeout=300
</span></span><span class="line"><span class="cl">SlurmdTimeout=30
</span></span><span class="line"><span class="cl">NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN
</span></span><span class="line"><span class="cl">NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN
</span></span><span class="line"><span class="cl">PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP</span></span></code></pre></div>
<p>Copy configuration to nodes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf
</span></span><span class="line"><span class="cl">scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf</span></span></code></pre></div>
<p>Start and enable services:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now slurmctld
</span></span><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now slurmd</span></span></code></pre></div>
</li>
<li>
<p><strong>Firewall Configuration:</strong></p>
<p>Open required ports:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6817/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6818/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6819/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --reload</span></span></code></pre></div>
</li>
</ol>
<h2 id="chapter-3-testing-and-introduction-to-the-commands"><strong>Chapter 3: Testing and Introduction to the commands:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-3-testing-and-introduction-to-the-commands">#</a></h2>
<p><strong>(While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)</strong></p>
<ol>
<li>
<p><code>sinfo</code>:</p>
<p>Displays node and partition information:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sinfo</span></span></code></pre></div>
</li>
<li>
<p><code>srun</code>:</p>
<p>Runs commands interactively on compute nodes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">srun -N2 -n2 nproc</span></span></code></pre></div>
</li>
<li>
<p><code>sbatch</code>:</p>
<p>Submits a job script:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sbatch testjob.sh</span></span></code></pre></div>
</li>
<li>
<p><code>squeue</code>:</p>
<p>Displays details of currently running jobs:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">squeue</span></span></code></pre></div>
</li>
<li>
<p><code>scancel</code>:</p>
<p>Cancels a submitted job:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scancel <span class="m">1</span></span></span></code></pre></div>
</li>
<li>
<p><code>scontrol</code>:</p>
<p>Displays detailed job and node information:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scontrol show job <span class="m">1</span></span></span></code></pre></div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scontrol show partition</span></span></code></pre></div>
</li>
</ol>
<h2 id="chapter-4-setting-up-the-nfs-storage"><strong>Chapter 4: Setting up the NFS storage.</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-4-setting-up-the-nfs-storage">#</a></h2>
<p>It is a good idea to have shared storage for SLURM. Install <code>nfs-utils</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo dnf install nfs-utils</span></span></code></pre></div>
<p><strong>On the NFS server:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir /srv/slurm_share
</span></span><span class="line"><span class="cl">nano /etc/exports</span></span></code></pre></div>
<p>Add the following line:</p>
<pre tabindex="0"><code>/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)
</code></pre><p>Open necessary ports:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">firewall-cmd --permanent --add-service<span class="o">=</span>rpc-bind
</span></span><span class="line"><span class="cl">firewall-cmd --permanent --add-port<span class="o">={</span>5555/tcp,5555/udp,6666/tcp,6666/udp<span class="o">}</span>
</span></span><span class="line"><span class="cl">firewall-cmd --reload</span></span></code></pre></div>
<p>Export and enable the service:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">exportfs -v
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> --now nfs-server</span></span></code></pre></div>
<p><strong>On the master and compute nodes:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir /mnt/slurm_share</span></span></code></pre></div>
<p>Add the mount in <code>/etc/fstab</code>:</p>
<pre tabindex="0"><code>10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0
</code></pre><p>Reboot machines and verify the share mounts properly.</p>
<h2 id="chapter-5-setting-up-the-compilebuild-environment"><strong>Chapter 5: Setting up the Compile/Build Environment</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-5-setting-up-the-compilebuild-environment">#</a></h2>
<p>Install kernel build dependencies:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">srun -n2 -N2 sudo dnf groupinstall <span class="s2">&#34;Development Tools&#34;</span> -y <span class="o">&amp;&amp;</span> sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y</span></span></code></pre></div>
<p>Download the Linux kernel source from <a href="https://kernel.org">kernel.org</a>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget <span class="o">[</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class="o">](</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class="o">)</span>
</span></span><span class="line"><span class="cl">tar xvf linux-6.14.8.tar.xz</span></span></code></pre></div>
<p>Define architecture-specific config:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make defconfig</span></span></code></pre></div>
<p>Create <code>compile_kernel.sh</code> on the shared directory:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="c1">#SBATCH --job-name=kernel_build</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --output=kernel_build_%j.out</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --error=kernel_build_%j.err</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --time=03:00:00</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --nodes=1</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --cpus-per-task=8</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --mem=8G</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">KERNEL_SOURCE_PATH</span><span class="o">=</span><span class="s2">&#34;/mnt/slurm_share/linux-6.8.9&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">BUILD_OUTPUT_DIR</span><span class="o">=</span><span class="s2">&#34;/mnt/slurm_share/kernel_builds/</span><span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">mkdir -p <span class="s2">&#34;</span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="nv">$KERNEL_SOURCE_PATH</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">NUM_MAKE_JOBS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">make -j<span class="s2">&#34;</span><span class="si">${</span><span class="nv">NUM_MAKE_JOBS</span><span class="si">}</span><span class="s2">&#34;</span> <span class="nv">ARCH</span><span class="o">=</span>x86_64 Image modules dtbs
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">cp <span class="s2">&#34;</span><span class="nv">$KERNEL_SOURCE_PATH</span><span class="s2">/arch/x86/boot/bzImage&#34;</span> <span class="s2">&#34;</span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Kernel compilation failed.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span></span></span></code></pre></div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on x"
            href="https://x.com/intent/tweet/?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&amp;title=SLURM%20as%20a%20Compilation%20Farm&amp;summary=SLURM%20as%20a%20Compilation%20Farm&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f&title=SLURM%20as%20a%20Compilation%20Farm">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on whatsapp"
            href="https://api.whatsapp.com/send?text=SLURM%20as%20a%20Compilation%20Farm%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on telegram"
            href="https://telegram.me/share/url?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=SLURM%20as%20a%20Compilation%20Farm&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>Â© Thamjeed</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>SLURM as a Compilation Farm | th4mjeed</title>
<meta name="keywords" content="">
<meta name="description" content="
ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.">
<meta name="author" content="Thamjeed">
<link rel="canonical" href="http://localhost:1313/posts/slurm-as-a-compilation-farm-old/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.46d7a9b8bdcee445fce1387b698d10c4568e9c8c10d43bf905cd321bc58d4099.css" integrity="sha256-RtepuL3O5EX84Th7aY0QxFaOnIwQ1Dv5Bc0yG8WNQJk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/slurm-as-a-compilation-farm-old/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/slurm-as-a-compilation-farm-old/">
  <meta property="og:site_name" content="th4mjeed">
  <meta property="og:title" content="SLURM as a Compilation Farm">
  <meta property="og:description" content=" ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-06T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SLURM as a Compilation Farm">
<meta name="twitter:description" content="
ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SLURM as a Compilation Farm",
      "item": "http://localhost:1313/posts/slurm-as-a-compilation-farm-old/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SLURM as a Compilation Farm",
  "name": "SLURM as a Compilation Farm",
  "description": " ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.\n",
  "keywords": [
    
  ],
  "articleBody": " ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of file, at least 30 Gigs is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with ssh, root login and defined ip of all 4 nodes in the /etc/hosts file\nThe Architecture diagram for the looks like this:\nChapter 1: The installation: Install and configure dependencies Installation of slurm requires EPEL repo to be installed across all instances, install and enable it via\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \"Development Tools\" sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge # Owner can rwx, group/others can rx (common for run dirs) On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key This creates a munge key with the necessary permissions, we need to copy the key to both our nodes by using scp.\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and Enable the service using:\nsudo systemctl enable --now munge Installation of SLURM Slurm is avaliable in the EPEL repo install on all 3 instances using:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not avaliable (Highly unlikely) you can always download tar file from Download Slurm - SchedMD extract it and use the below command to compile and install it.\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration On all 3 instances use the following command\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm This creates the user, log files and changes the ownership of the files to be slurm\nNow open the slurm config file on the master using:\nsudo nano /etc/slurm/slurm.conf The main lines to be focused on are:\nClusterName=debug\nSlurmUser=slurm\nControlMachine=slurm-master\nSlurmctldPort=6817\nSlurmdPort=6818\nAuthType=auth/munge\nStateSaveLocation=/var/spool/slurmctld\nSlurmdSpoolDir=/var/spool/slurmd\nSwitchType=switch/none\nMpiDefault=none\nSlurmctldPidFile=/var/run/slurmctld.pid\nSlurmdPidFile=/var/run/slurmd.pid\nProctrackType=proctrack/pgid\nReturnToService=1SchedulerType=sched/backfill\nSlurmctldTimeout=300\nSlurmdTimeout=30\nNodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN\nNodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN\nPartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP\nMake sure to the values are the same as above without any spelling mistakes, also if any value is not available create them.\nCopy the same config onto node1 and node2 using\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable the services on boot\nOn master:\nsudo systemctl enable --now slurmctld On compute nodes:\nsudo systemctl enable --now slurmd Slurm should be working and synced up to the nodes\nFirewall Configuration: Firewall prevents ports required by slurm to be accessed, while in my testing case I had disable the firewall service, I would not recommend doing this in production, hence the correct way to open ports in slurm, on all 3 instances:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: [While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply]\nsinfo: This command is displays the information about the nodes, what the partition name is, on what state theyâ€™re in and how many nodes are included in the partition\n[root@master ~]# sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 2 idle node[1-2] Idle here means that the nodes are waiting for a job to be assigned.\nsrun: srun is used to run command on the compute nodes via master in an interactive manner. For example if I needed to know the cores available in node 1 and 2\n[root@master ~]# srun -N2 -n2 nproc 4 8 Where the -N is the number of nodes for the job (here nproc) to be allocated, and -n is the number of tasks to be given to the node.\nsbatch : Used to submit a job script which is provided as an argument to the file. For example\n[root@master ~]# sbatch [testjob.sh](http://testjob.sh/) Submitted batch job 1 Weâ€™ll be learning more about the syntax of these files later\nsqueue: Used to display details of currently running jobs, such as on what node is it running, user and how long has it has been running.\n[root@master ~]# squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1 debug test_job root R 0:02 1 node1 Here we can see the job we previously executed job via sbatch\nscancel: Used to cancel a submitted job, this command does not explicitly return an output.\n[root@master ~]# scancel 1 [root@master ~]# squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) [root@master ~]# scontrol : Provides a detailed information about the nodes the usage of this command is a little vast some common example are:\n[root@master ~]# scontrol show job 1 JobId=1 JobName=test_job UserId=root(0) GroupId=root(0) MCS_label=N/A Priority=4294901750 Nice=0 Account=(null) QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=00:00:04 TimeLimit=00:01:00 TimeMin=N/A SubmitTime=2025-05-19T02:34:05 EligibleTime=2025-05-19T02:34:05 AccrueTime=2025-05-19T02:34:05 StartTime=2025-05-19T02:34:05 EndTime=2025-05-19T02:35:05 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-05-19T02:34:05 Scheduler=Backfill Partition=debug AllocNode:Sid=master:20850 ReqNodeList=(null) ExcNodeList=(null) NodeList=node1 BatchHost=node1 NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:* TRES=cpu=1,node=1,billing=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0 Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/root/testjob.sh WorkDir=/root StdErr=/root/slurm_test_output.txt StdIn=/dev/null StdOut=/root/slurm_test_output.txt Power= 6.1 scontrol show job 1: Displays details of the running job\n[root@master ~]# scontrol show partition PartitionName=debug AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL AllocNodes=ALL Default=YES QoS=N/A DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED Nodes=node[1-2] PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=12 TotalNodes=2 SelectTypeParameters=NONE JobDefaults=(null) DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED TRES=cpu=12,mem=11339M,node=2,billing=12 6.2 scontrol show partition: Displays information about the partitions (here referring to the node config).\n6.3 scontrol cancel jobid=: Another way to cancel jobs\n6.4 scontrol requeue : Requeues the an errored out job\n6.5 scontrol update jobid= priority=100000: Increases the priority of a running job\n6.6 scontrol update NodeName=node01 State=RESUME: Resume a drained or a disabled node\nChapter 4: Setting up the NFS storage. It is a good idea to have a shared storage on slurm, since you may run into issues like not having enough space, or when you want both the nodes accessing a same file or folder. Weâ€™ll be using nfs-utils package to achieve this, install on NFS device and\nsudo dnf install nfs-utils On the Linux machine to be used as NFS (here testing-server)\nCreating a directory in /srv/slrum_share using:\nmkdir /srv/slurm_share Open the file /etc/exports in nano and add the following line\nnano /etc/exports /srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)\nSave and quit the file.\nOpen Ports using:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export the share and enable the server using the following command:\nexportfs -v systemctl enable â€”now nfs-server On the master and compute nodes:\nCreate a directory in /mnt/slurm_share using:\nsudo mkdir /mnt/slurm_share To the mount the share on boot open the following file in nano with /etc/nano and paste the line:\n/mnt/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)\nReboot the machines and you should have the shared folder mounted to /mnt/slurm_share\nChapter 5: Setting up the Compile/Build Environment There are certain development dependencies that are required to build the kernel, these need to be installed on all the nodes including the master, use this command to parallelly install them:\nsrun -n2 -N2 sudo dnf groupinstall \"Development Tools\" -y \u0026\u0026 sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y This will install them in on the nodes and the same command without srun -n2 -N2 to install on master.\nDownload the Linux kernel source from kernel.org as for writing this the latest stable kernel version is 6.14.8. Download this tar and extract on the shared folder using.\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Use the below command to define an architecture specific .config file\nmake defconfig On master:\ncreate a file named compile_kernel.sh on the share mounted directory and paste the code in it:\n#!/bin/bash # # SLURM Batch Script for Kernel Compilation # #SBATCH --job-name=kernel_build # Name of your job #SBATCH --output=kernel_build_%j.out # Standard output file (%j expands to jobID) #SBATCH --error=kernel_build_%j.err # Standard error file #SBATCH --time=03:00:00 # Max runtime (3 hours, adjust as needed) #SBATCH --nodes=1 # Request 1 compute node # # --- RESOURCE REQUESTS (CHOOSE ONE SET) --- # # Option A: For node1 (4 cores, 4GB RAM) # #SBATCH --cpus-per-task=4 # Request 4 CPU cores for make -j4 # #SBATCH --mem=4G # Request 4GB RAM # # Option B: For node2 (8 cores, 8GB RAM) - RECOMMENDED FOR FASTER BUILDS #SBATCH --cpus-per-task=8 # Request 8 CPU cores for make -j8 #SBATCH --mem=8G # Request 8GB RAM # # --- END RESOURCE REQUESTS --- # Set environment variables for the kernel build # Replace 'linux-6.x.y' with your actual kernel source directory name KERNEL_SOURCE_PATH=\"/mnt/slurm_share/linux-6.8.9\" BUILD_OUTPUT_DIR=\"/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\" echo \"-----------------------------------------------------\" echo \"SLURM Job ID: ${SLURM_JOB_ID}\" echo \"Job Name: ${SLURM_JOB_NAME}\" echo \"Running on host: $(hostname)\" echo \"Assigned CPUS: ${SLURM_CPUS_PER_TASK}\" echo \"Assigned Memory: ${SLURM_MEM_PER_NODE}MB\" # Note: SLURM_MEM_PER_NODE is in MB if --mem was in GB echo \"Kernel Source Path: ${KERNEL_SOURCE_PATH}\" echo \"Build Output Dir: ${BUILD_OUTPUT_DIR}\" echo \"Current working directory: $(pwd)\" echo \"-----------------------------------------------------\" # Create a unique directory for the build output (artifacts) mkdir -p \"$BUILD_OUTPUT_DIR\" if [ $? -ne 0 ]; then echo \"ERROR: Could not create output directory $BUILD_OUTPUT_DIR\" exit 1 fi # Change to the kernel source directory cd \"$KERNEL_SOURCE_PATH\" if [ $? -ne 0 ]; then echo \"ERROR: Could not change to kernel source directory $KERNEL_SOURCE_PATH\" exit 1 fi # Load any necessary modules (e.g., specific GCC versions, cross-compilers) # module load gcc/12.2.0 # module load some-toolchain # Set the number of parallel jobs for 'make'. # This uses the number of CPUs SLURM allocated to your job. NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} echo \"Starting kernel compilation using 'make -j${NUM_MAKE_JOBS}'...\" echo \"Compiling on node: $(hostname) with $(nproc) physical cores and $(lscpu | grep 'Thread(s) per core' | awk '{print $NF}') threads per core.\" # Execute the kernel compilation # Assuming you want to build the 'Image' (compressed kernel), 'modules', and 'device tree blobs' # Adjust ARCH and CROSS_COMPILE if you are cross-compiling for a different architecture make -j\"${NUM_MAKE_JOBS}\" \\ ARCH=x86_64 \\ # CROSS_COMPILE=arm-linux-gnueabihf- \\ # Uncomment and adjust for cross-compilation Image modules dtbs # Check the exit status of the make command if [ $? -eq 0 ]; then echo \"Kernel compilation successful!\" echo \"Copying build artifacts to $BUILD_OUTPUT_DIR\" # Copy relevant build artifacts to your output directory cp \"$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\" \"$BUILD_OUTPUT_DIR/\" # For modules, you might want to install them into a temporary location # make modules_install INSTALL_MOD_PATH=\"${BUILD_OUTPUT_DIR}/modules_install\" # cp -r \"$KERNEL_SOURCE_PATH/System.map\" \"$BUILD_OUTPUT_DIR/\" # cp -r \"$KERNEL_SOURCE_PATH/.config\" \"$BUILD_OUTPUT_DIR/\" else echo \"Kernel compilation FAILED. Check the output in ${SLURM_JOB_NAME}_${SLURM_JOB_ID}.err\" echo \"Also check the detailed logs in $BUILD_OUTPUT_DIR/.\" fi echo \"Job finished at $(date)\" echo \"-----------------------------------------------------\" ",
  "wordCount" : "1917",
  "inLanguage": "en",
  "datePublished": "2025-11-06T00:00:00Z",
  "dateModified": "2025-11-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Thamjeed"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/slurm-as-a-compilation-farm-old/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "th4mjeed",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="th4mjeed (Alt + H)">th4mjeed</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      SLURM as a Compilation Farm
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-11-06 00:00:00 +0000 UTC'>November 6, 2025</span>&nbsp;Â·&nbsp;<span>9 min</span>&nbsp;Â·&nbsp;<span>Thamjeed</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction:">Introduction:</a></li>
                <li>
                    <a href="#lab-infrastructure" aria-label="Lab Infrastructure:">Lab Infrastructure:</a></li>
                <li>
                    <a href="#chapter-1-the-installation" aria-label="Chapter 1: The installation:">Chapter 1: The installation:</a></li>
                <li>
                    <a href="#chapter-2-the-configuration" aria-label="Chapter 2: The Configuration:">Chapter 2: The Configuration:</a></li>
                <li>
                    <a href="#chapter-3-testing-and-introduction-to-the-commands" aria-label="Chapter 3: Testing and Introduction to the commands:">Chapter 3: Testing and Introduction to the commands:</a></li>
                <li>
                    <a href="#chapter-4-setting-up-the-nfs-storage" aria-label="Chapter 4: Setting up the NFS storage.">Chapter 4: Setting up the NFS storage.</a></li>
                <li>
                    <a href="#chapter-5-setting-up-the-compilebuild-environment" aria-label="Chapter 5: Setting up the Compile/Build Environment">Chapter 5: Setting up the Compile/Build Environment</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>ðŸ“¢ NOTE: This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic, take this as a practical guide from a noobâ€™s perspective diving into it.</p></blockquote>
<h2 id="introduction"><strong>Introduction:</strong><a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also me used to compile any other tool given that the perquisites and dependencies are known.</p>
<h2 id="lab-infrastructure"><strong>Lab Infrastructure:</strong><a hidden class="anchor" aria-hidden="true" href="#lab-infrastructure">#</a></h2>
<p>The following are all on VMware ESXI</p>
<ol>
<li>
<p><strong>Master:</strong></p>
<ul>
<li>CPUs 4</li>
<li>Memory 4 GB</li>
<li>Hard disk 20 GB</li>
<li>Hostname: master</li>
</ul>
</li>
<li>
<p><strong>Node 1:</strong></p>
<ul>
<li>CPUs  4</li>
<li>Memory 4 GB</li>
<li>Hard disk 40 GB</li>
<li>Hostname: node1</li>
</ul>
</li>
<li>
<p><strong>Node 2:</strong></p>
<ul>
<li>CPUs 8</li>
<li>Memory 8 GB</li>
<li>Hard disk 40 GB</li>
<li>Hostname: node2</li>
</ul>
</li>
<li>
<p><strong>Network File Storage</strong></p>
<ul>
<li>Since compiling creates dozens of file, at least 30 Gigs is required for a successful compilation.</li>
<li>Used the existing testing server assigned to me.</li>
<li>NFS share path located in <strong>/mnt/slrum_share</strong></li>
</ul>
</li>
</ol>
<p>Every instance has Rocky Linux 9.5 installed with ssh, root login and defined ip of all 4 nodes in the /etc/hosts file</p>
<p>The Architecture diagram for the looks like this:</p>
<!-- raw HTML omitted -->
<h2 id="chapter-1-the-installation"><strong>Chapter 1: The installation:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-1-the-installation">#</a></h2>
<ol>
<li><strong>Install and configure dependencies</strong></li>
</ol>
<p>Installation of slurm requires EPEL repo to be installed across all instances, install and enable it via</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">dnf config-manager --set-enabled crb
</span></span><span class="line"><span class="cl">dnf install epel-release
</span></span><span class="line"><span class="cl">sudo dnf groupinstall <span class="s2">&#34;Development Tools&#34;</span>
</span></span><span class="line"><span class="cl">sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel
</span></span></code></pre></div><p>MUNGE is an authentication mechanism for secure communication between Slurm components. configure it on all instances using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo useradd munge
</span></span><span class="line"><span class="cl">sudo mkdir -p /etc/munge /var/log/munge /var/run/munge
</span></span><span class="line"><span class="cl">sudo chown munge:munge /usr/local/var/run/munge
</span></span><span class="line"><span class="cl">sudo chmod <span class="m">0755</span> /usr/local/var/run/munge <span class="c1"># Owner can rwx, group/others can rx (common for run dirs)</span>
</span></span></code></pre></div><p>On Master:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo /usr/sbin/create-munge-key
</span></span><span class="line"><span class="cl">sudo chown munge:munge /etc/munge/munge.key
</span></span><span class="line"><span class="cl">sudo chmod <span class="m">0400</span> /etc/munge/munge.key
</span></span></code></pre></div><p>This creates a munge key with the necessary permissions, we need to copy the key to both our nodes by using scp.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scp /etc/munge/munge.key root@node1:/etc/munge/
</span></span><span class="line"><span class="cl">scp /etc/munge/munge.key root@node2:/etc/munge/
</span></span></code></pre></div><p>Start and Enable the service using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now munge
</span></span></code></pre></div><ol>
<li><strong>Installation of SLURM</strong></li>
</ol>
<p>Slurm is avaliable in the EPEL repo install on all 3 instances using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi
</span></span></code></pre></div><p>If by any chance packages are not avaliable (Highly unlikely) you can always download tar file from <a href="https://www.schedmd.com/download-slurm/">Download Slurm - SchedMD</a> extract it and use the below command to compile and install it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make -j<span class="k">$(</span>nproc<span class="k">)</span>
</span></span><span class="line"><span class="cl">sudo make install
</span></span></code></pre></div><h2 id="chapter-2-the-configuration"><strong>Chapter 2: The Configuration:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-2-the-configuration">#</a></h2>
<ol>
<li><strong>Slurm configuration</strong></li>
</ol>
<p>On all 3 instances use the following command</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo useradd slurm
</span></span><span class="line"><span class="cl">sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
</span></span><span class="line"><span class="cl">sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
</span></span></code></pre></div><p>This creates the user, log files and changes the ownership of the files to be slurm</p>
<p>Now open the slurm config file on the master using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sudo nano /etc/slurm/slurm.conf
</span></span></code></pre></div><p>The main lines to be focused on are:</p>
<p><code>ClusterName=debug</code></p>
<p><code>SlurmUser=slurm</code></p>
<p><code>ControlMachine=slurm-master</code></p>
<p><code>SlurmctldPort=6817</code></p>
<p><code>SlurmdPort=6818</code></p>
<p><code>AuthType=auth/munge</code></p>
<p><code>StateSaveLocation=/var/spool/slurmctld</code></p>
<p><code>SlurmdSpoolDir=/var/spool/slurmd</code></p>
<p><code>SwitchType=switch/none</code></p>
<p><code>MpiDefault=none</code></p>
<p><code>SlurmctldPidFile=/var/run/slurmctld.pid</code></p>
<p><code>SlurmdPidFile=/var/run/slurmd.pid</code></p>
<p><code>ProctrackType=proctrack/pgid</code></p>
<p><code>ReturnToService=1SchedulerType=sched/backfill</code></p>
<p><code>SlurmctldTimeout=300</code></p>
<p><code>SlurmdTimeout=30</code></p>
<p><code>NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN</code></p>
<p><code>NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN</code></p>
<p><code>PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP</code></p>
<p>Make sure to the values are the same as above without any spelling mistakes, also if any value is not available create them.</p>
<p>Copy the same config onto <strong>node1</strong> and <strong>node2 using</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf
</span></span><span class="line"><span class="cl">scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf
</span></span></code></pre></div><p>Start and enable the services on boot</p>
<p>On master:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now slurmctld
</span></span></code></pre></div><p>On compute nodes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo systemctl <span class="nb">enable</span> --now slurmd
</span></span></code></pre></div><p>Slurm should be working and synced up to the nodes</p>
<ol>
<li><strong>Firewall Configuration:</strong></li>
</ol>
<p>Firewall prevents ports required by slurm to be accessed, while in my testing case I had disable the firewall service, I would not recommend doing this in production, hence the correct way to open ports in slurm, on all 3 instances:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6817/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6818/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --permanent --add-port<span class="o">=</span>6819/tcp
</span></span><span class="line"><span class="cl">sudo firewall-cmd --reload
</span></span></code></pre></div><h2 id="chapter-3-testing-and-introduction-to-the-commands"><strong>Chapter 3: Testing and Introduction to the commands:</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-3-testing-and-introduction-to-the-commands">#</a></h2>
<p><strong>[While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply]</strong></p>
<ol>
<li><code>sinfo</code>:</li>
</ol>
<p>This command is displays the information about the nodes, what the partition name is, on what state theyâ€™re in and how many nodes are included in the partition</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># sinfo</span>
</span></span><span class="line"><span class="cl">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
</span></span><span class="line"><span class="cl">debug*       up   infinite      <span class="m">2</span>   idle node<span class="o">[</span>1-2<span class="o">]</span>
</span></span></code></pre></div><p>Idle here means that the nodes are waiting for a job to be assigned.</p>
<ol>
<li><code>srun</code>:</li>
</ol>
<p>srun is used to run command on the compute nodes via master in an interactive manner. For example if I needed to know the cores available in node 1 and 2</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># srun -N2 -n2 nproc</span>
</span></span><span class="line"><span class="cl"> <span class="m">4</span>
</span></span><span class="line"><span class="cl"> <span class="m">8</span>
</span></span></code></pre></div><p>Where the <code>-N</code>  is the number of nodes for the job (here nproc) to be allocated, and <code>-n</code> is the number of tasks to be given to the node.</p>
<ol>
<li><code>sbatch</code> :</li>
</ol>
<p>Used to submit a job script which is provided as an argument to the file. For example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># sbatch [testjob.sh](http://testjob.sh/)</span>
</span></span><span class="line"><span class="cl">Submitted batch job <span class="m">1</span>
</span></span></code></pre></div><p>Weâ€™ll be learning more about the syntax of these files later</p>
<ol>
<li><code>squeue</code>:</li>
</ol>
<p>Used to display details of currently running jobs, such as on what node is it running, user and how long has it has been running.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># squeue</span>
</span></span><span class="line"><span class="cl">JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
</span></span><span class="line"><span class="cl">	<span class="m">1</span>      debug   test_job    root  R       0:02      <span class="m">1</span> node1
</span></span></code></pre></div><p>Here we can see the job we previously executed job via <code>sbatch</code></p>
<ol>
<li><code>scancel</code>:</li>
</ol>
<p>Used to cancel a submitted job, this command does not explicitly return an output.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># scancel 1</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># squeue</span>
</span></span><span class="line"><span class="cl">JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1">#</span>
</span></span></code></pre></div><ol>
<li><code>scontrol</code> :</li>
</ol>
<p>Provides a detailed information about the nodes the usage of this command is a little vast some common example are:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># scontrol show job 1</span>
</span></span><span class="line"><span class="cl"><span class="nv">JobId</span><span class="o">=</span><span class="m">1</span> <span class="nv">JobName</span><span class="o">=</span>test_job
</span></span><span class="line"><span class="cl"><span class="nv">UserId</span><span class="o">=</span>root<span class="o">(</span>0<span class="o">)</span> <span class="nv">GroupId</span><span class="o">=</span>root<span class="o">(</span>0<span class="o">)</span> <span class="nv">MCS_label</span><span class="o">=</span>N/A
</span></span><span class="line"><span class="cl"><span class="nv">Priority</span><span class="o">=</span><span class="m">4294901750</span> <span class="nv">Nice</span><span class="o">=</span><span class="m">0</span> <span class="nv">Account</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">QOS</span><span class="o">=</span>normal
</span></span><span class="line"><span class="cl"><span class="nv">JobState</span><span class="o">=</span>RUNNING <span class="nv">Reason</span><span class="o">=</span>None <span class="nv">Dependency</span><span class="o">=(</span>null<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">Requeue</span><span class="o">=</span><span class="m">1</span> <span class="nv">Restarts</span><span class="o">=</span><span class="m">0</span> <span class="nv">BatchFlag</span><span class="o">=</span><span class="m">1</span> <span class="nv">Reboot</span><span class="o">=</span><span class="m">0</span> <span class="nv">ExitCode</span><span class="o">=</span>0:0
</span></span><span class="line"><span class="cl"><span class="nv">RunTime</span><span class="o">=</span>00:00:04 <span class="nv">TimeLimit</span><span class="o">=</span>00:01:00 <span class="nv">TimeMin</span><span class="o">=</span>N/A
</span></span><span class="line"><span class="cl"><span class="nv">SubmitTime</span><span class="o">=</span>2025-05-19T02:34:05 <span class="nv">EligibleTime</span><span class="o">=</span>2025-05-19T02:34:05
</span></span><span class="line"><span class="cl"><span class="nv">AccrueTime</span><span class="o">=</span>2025-05-19T02:34:05
</span></span><span class="line"><span class="cl"><span class="nv">StartTime</span><span class="o">=</span>2025-05-19T02:34:05 <span class="nv">EndTime</span><span class="o">=</span>2025-05-19T02:35:05 <span class="nv">Deadline</span><span class="o">=</span>N/A
</span></span><span class="line"><span class="cl"><span class="nv">SuspendTime</span><span class="o">=</span>None <span class="nv">SecsPreSuspend</span><span class="o">=</span><span class="m">0</span> <span class="nv">LastSchedEval</span><span class="o">=</span>2025-05-19T02:34:05 <span class="nv">Scheduler</span><span class="o">=</span>Backfill
</span></span><span class="line"><span class="cl"><span class="nv">Partition</span><span class="o">=</span>debug AllocNode:Sid<span class="o">=</span>master:20850
</span></span><span class="line"><span class="cl"><span class="nv">ReqNodeList</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">ExcNodeList</span><span class="o">=(</span>null<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">NodeList</span><span class="o">=</span>node1
</span></span><span class="line"><span class="cl"><span class="nv">BatchHost</span><span class="o">=</span>node1
</span></span><span class="line"><span class="cl"><span class="nv">NumNodes</span><span class="o">=</span><span class="m">1</span> <span class="nv">NumCPUs</span><span class="o">=</span><span class="m">1</span> <span class="nv">NumTasks</span><span class="o">=</span><span class="m">1</span> CPUs/Task<span class="o">=</span><span class="m">1</span> ReqB:S:C:T<span class="o">=</span>0:0:*:*
</span></span><span class="line"><span class="cl"><span class="nv">TRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>1,node<span class="o">=</span>1,billing<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">Socks/Node<span class="o">=</span>* NtasksPerN:B:S:C<span class="o">=</span>0:0:*:* <span class="nv">CoreSpec</span><span class="o">=</span>*
</span></span><span class="line"><span class="cl"><span class="nv">MinCPUsNode</span><span class="o">=</span><span class="m">1</span> <span class="nv">MinMemoryNode</span><span class="o">=</span><span class="m">0</span> <span class="nv">MinTmpDiskNode</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="nv">Features</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">DelayBoot</span><span class="o">=</span>00:00:00
</span></span><span class="line"><span class="cl"><span class="nv">OverSubscribe</span><span class="o">=</span>OK <span class="nv">Contiguous</span><span class="o">=</span><span class="m">0</span> <span class="nv">Licenses</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">Network</span><span class="o">=(</span>null<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">Command</span><span class="o">=</span>/root/testjob.sh
</span></span><span class="line"><span class="cl"><span class="nv">WorkDir</span><span class="o">=</span>/root
</span></span><span class="line"><span class="cl"><span class="nv">StdErr</span><span class="o">=</span>/root/slurm_test_output.txt
</span></span><span class="line"><span class="cl"><span class="nv">StdIn</span><span class="o">=</span>/dev/null
</span></span><span class="line"><span class="cl"><span class="nv">StdOut</span><span class="o">=</span>/root/slurm_test_output.txt
</span></span><span class="line"><span class="cl"><span class="nv">Power</span><span class="o">=</span>
</span></span></code></pre></div><p>6.1 <code>scontrol show job 1</code>: Displays details of the running job</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@master ~<span class="o">]</span><span class="c1"># scontrol show partition</span>
</span></span><span class="line"><span class="cl"><span class="nv">PartitionName</span><span class="o">=</span>debug
</span></span><span class="line"><span class="cl"><span class="nv">AllowGroups</span><span class="o">=</span>ALL <span class="nv">AllowAccounts</span><span class="o">=</span>ALL <span class="nv">AllowQos</span><span class="o">=</span>ALL
</span></span><span class="line"><span class="cl"><span class="nv">AllocNodes</span><span class="o">=</span>ALL <span class="nv">Default</span><span class="o">=</span>YES <span class="nv">QoS</span><span class="o">=</span>N/A
</span></span><span class="line"><span class="cl"><span class="nv">DefaultTime</span><span class="o">=</span>NONE <span class="nv">DisableRootJobs</span><span class="o">=</span>NO <span class="nv">ExclusiveUser</span><span class="o">=</span>NO <span class="nv">GraceTime</span><span class="o">=</span><span class="m">0</span> <span class="nv">Hidden</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">MaxNodes</span><span class="o">=</span>UNLIMITED <span class="nv">MaxTime</span><span class="o">=</span>UNLIMITED <span class="nv">MinNodes</span><span class="o">=</span><span class="m">0</span> <span class="nv">LLN</span><span class="o">=</span>NO <span class="nv">MaxCPUsPerNode</span><span class="o">=</span>UNLIMITED
</span></span><span class="line"><span class="cl"><span class="nv">Nodes</span><span class="o">=</span>node<span class="o">[</span>1-2<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">PriorityJobFactor</span><span class="o">=</span><span class="m">1</span> <span class="nv">PriorityTier</span><span class="o">=</span><span class="m">1</span> <span class="nv">RootOnly</span><span class="o">=</span>NO <span class="nv">ReqResv</span><span class="o">=</span>NO <span class="nv">OverSubscribe</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">OverTimeLimit</span><span class="o">=</span>NONE <span class="nv">PreemptMode</span><span class="o">=</span>OFF
</span></span><span class="line"><span class="cl"><span class="nv">State</span><span class="o">=</span>UP <span class="nv">TotalCPUs</span><span class="o">=</span><span class="m">12</span> <span class="nv">TotalNodes</span><span class="o">=</span><span class="m">2</span> <span class="nv">SelectTypeParameters</span><span class="o">=</span>NONE
</span></span><span class="line"><span class="cl"><span class="nv">JobDefaults</span><span class="o">=(</span>null<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">DefMemPerNode</span><span class="o">=</span>UNLIMITED <span class="nv">MaxMemPerNode</span><span class="o">=</span>UNLIMITED
</span></span><span class="line"><span class="cl"><span class="nv">TRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>12,mem<span class="o">=</span>11339M,node<span class="o">=</span>2,billing<span class="o">=</span><span class="m">12</span>
</span></span></code></pre></div><p>6.2 <code>scontrol show partition</code>: Displays information about the partitions (here referring to the node config).</p>
<p>6.3 <code>scontrol cancel jobid=&lt;jobid&gt;</code>: Another way to cancel jobs</p>
<p>6.4 <code>scontrol requeue &lt;jobid&gt;</code>: Requeues the an errored out job</p>
<p>6.5 <code>scontrol update jobid=&lt;job_id&gt; priority=100000</code>: Increases the priority of a running job</p>
<p>6.6 <code>scontrol update NodeName=node01 State=RESUME</code>: Resume a drained or a disabled node</p>
<h2 id="chapter-4-setting-up-the-nfs-storage"><strong>Chapter 4: Setting up the NFS storage.</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-4-setting-up-the-nfs-storage">#</a></h2>
<p>It is a good idea to have a shared storage on slurm, since you may run into issues like not having enough space, or when you want both the nodes accessing a same file or folder. Weâ€™ll be using nfs-utils package to achieve this, install on NFS device and</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo dnf install nfs-utils
</span></span></code></pre></div><p><strong>On the Linux machine to be used as NFS (here testing-server)</strong></p>
<p>Creating a directory in /srv/slrum_share using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir /srv/slurm_share
</span></span></code></pre></div><p>Open the file <strong>/etc/exports</strong> in nano and add the following line</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">nano /etc/exports
</span></span></code></pre></div><p><code>/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)</code></p>
<p>Save and quit the file.</p>
<p>Open Ports using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">firewall-cmd --permanent --add-service<span class="o">=</span>rpc-bind
</span></span><span class="line"><span class="cl">firewall-cmd --permanent --add-port<span class="o">={</span>5555/tcp,5555/udp,6666/tcp,6666/udp<span class="o">}</span>
</span></span><span class="line"><span class="cl">firewall-cmd --reload
</span></span></code></pre></div><p>Export the share and enable the server using the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">exportfs -v   
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> â€”now nfs-server 
</span></span></code></pre></div><p><strong>On the master and compute nodes:</strong></p>
<p>Create a directory in /mnt/slurm_share using:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir /mnt/slurm_share
</span></span></code></pre></div><p>To the mount the share on boot open the following file in nano with /etc/nano and paste the line:</p>
<p><code>/mnt/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)</code></p>
<p>Reboot the machines and you should have the shared folder mounted to <strong>/mnt/slurm_share</strong></p>
<h2 id="chapter-5-setting-up-the-compilebuild-environment"><strong>Chapter 5: Setting up the Compile/Build Environment</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-5-setting-up-the-compilebuild-environment">#</a></h2>
<p>There are certain development dependencies that are required to build the kernel, these need to be installed on all the nodes including the master, use this command to parallelly install them:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">srun -n2 -N2 sudo dnf groupinstall <span class="s2">&#34;Development Tools&#34;</span> -y <span class="o">&amp;&amp;</span>  sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y
</span></span></code></pre></div><p>This will install them in on the nodes and the same command without <strong>srun -n2 -N2</strong> to install on master.</p>
<p>Download the Linux kernel source from <a href="http://kernel.org">kernel.org</a> as for writing this the latest stable kernel version is 6.14.8. Download this tar and extract on the shared folder using.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget <span class="o">[</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class="o">](</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span class="o">)</span>
</span></span><span class="line"><span class="cl">tar xvf linux-6.14.8.tar.xz
</span></span></code></pre></div><p>Use the below command to define an architecture specific <strong>.config</strong> file</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make defconfig
</span></span></code></pre></div><p><strong>On master:</strong></p>
<p>create a file named compile_kernel.sh on the share mounted directory and paste the code in it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># SLURM Batch Script for Kernel Compilation</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --job-name=kernel_build       # Name of your job</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --output=kernel_build_%j.out  # Standard output file (%j expands to jobID)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --error=kernel_build_%j.err   # Standard error file</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --time=03:00:00               # Max runtime (3 hours, adjust as needed)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --nodes=1                     # Request 1 compute node</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># --- RESOURCE REQUESTS (CHOOSE ONE SET) ---</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Option A: For node1 (4 cores, 4GB RAM)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># #SBATCH --cpus-per-task=4           # Request 4 CPU cores for make -j4</span>
</span></span><span class="line"><span class="cl"><span class="c1"># #SBATCH --mem=4G                    # Request 4GB RAM</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Option B: For node2 (8 cores, 8GB RAM) - RECOMMENDED FOR FASTER BUILDS</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --cpus-per-task=8             # Request 8 CPU cores for make -j8</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SBATCH --mem=8G                      # Request 8GB RAM</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># --- END RESOURCE REQUESTS ---</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set environment variables for the kernel build</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Replace &#39;linux-6.x.y&#39; with your actual kernel source directory name</span>
</span></span><span class="line"><span class="cl"><span class="nv">KERNEL_SOURCE_PATH</span><span class="o">=</span><span class="s2">&#34;/mnt/slurm_share/linux-6.8.9&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">BUILD_OUTPUT_DIR</span><span class="o">=</span><span class="s2">&#34;/mnt/slurm_share/kernel_builds/</span><span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;-----------------------------------------------------&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;SLURM Job ID: </span><span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Job Name: </span><span class="si">${</span><span class="nv">SLURM_JOB_NAME</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Running on host: </span><span class="k">$(</span>hostname<span class="k">)</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Assigned CPUS: </span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Assigned Memory: </span><span class="si">${</span><span class="nv">SLURM_MEM_PER_NODE</span><span class="si">}</span><span class="s2">MB&#34;</span> <span class="c1"># Note: SLURM_MEM_PER_NODE is in MB if --mem was in GB</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Kernel Source Path: </span><span class="si">${</span><span class="nv">KERNEL_SOURCE_PATH</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Build Output Dir: </span><span class="si">${</span><span class="nv">BUILD_OUTPUT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Current working directory: </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;-----------------------------------------------------&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a unique directory for the build output (artifacts)</span>
</span></span><span class="line"><span class="cl">mkdir -p <span class="s2">&#34;</span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;ERROR: Could not create output directory </span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">exit</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Change to the kernel source directory</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="nv">$KERNEL_SOURCE_PATH</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;ERROR: Could not change to kernel source directory </span><span class="nv">$KERNEL_SOURCE_PATH</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">exit</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load any necessary modules (e.g., specific GCC versions, cross-compilers)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># module load gcc/12.2.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># module load some-toolchain</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the number of parallel jobs for &#39;make&#39;.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This uses the number of CPUs SLURM allocated to your job.</span>
</span></span><span class="line"><span class="cl"><span class="nv">NUM_MAKE_JOBS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Starting kernel compilation using &#39;make -j</span><span class="si">${</span><span class="nv">NUM_MAKE_JOBS</span><span class="si">}</span><span class="s2">&#39;...&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Compiling on node: </span><span class="k">$(</span>hostname<span class="k">)</span><span class="s2"> with </span><span class="k">$(</span>nproc<span class="k">)</span><span class="s2"> physical cores and </span><span class="k">$(</span>lscpu <span class="p">|</span> grep <span class="s1">&#39;Thread(s) per core&#39;</span> <span class="p">|</span> awk <span class="s1">&#39;{print $NF}&#39;</span><span class="k">)</span><span class="s2"> threads per core.&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Execute the kernel compilation</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Assuming you want to build the &#39;Image&#39; (compressed kernel), &#39;modules&#39;, and &#39;device tree blobs&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Adjust ARCH and CROSS_COMPILE if you are cross-compiling for a different architecture</span>
</span></span><span class="line"><span class="cl">make -j<span class="s2">&#34;</span><span class="si">${</span><span class="nv">NUM_MAKE_JOBS</span><span class="si">}</span><span class="s2">&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    <span class="nv">ARCH</span><span class="o">=</span>x86_64 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    <span class="c1"># CROSS_COMPILE=arm-linux-gnueabihf- \ # Uncomment and adjust for cross-compilation</span>
</span></span><span class="line"><span class="cl">    Image modules dtbs
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check the exit status of the make command</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;Kernel compilation successful!&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;Copying build artifacts to </span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Copy relevant build artifacts to your output directory</span>
</span></span><span class="line"><span class="cl">    cp <span class="s2">&#34;</span><span class="nv">$KERNEL_SOURCE_PATH</span><span class="s2">/arch/x86/boot/bzImage&#34;</span> <span class="s2">&#34;</span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># For modules, you might want to install them into a temporary location</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># make modules_install INSTALL_MOD_PATH=&#34;${BUILD_OUTPUT_DIR}/modules_install&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># cp -r &#34;$KERNEL_SOURCE_PATH/System.map&#34; &#34;$BUILD_OUTPUT_DIR/&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># cp -r &#34;$KERNEL_SOURCE_PATH/.config&#34; &#34;$BUILD_OUTPUT_DIR/&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">else</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;Kernel compilation FAILED. Check the output in </span><span class="si">${</span><span class="nv">SLURM_JOB_NAME</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span><span class="s2">.err&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;Also check the detailed logs in </span><span class="nv">$BUILD_OUTPUT_DIR</span><span class="s2">/.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Job finished at </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;-----------------------------------------------------&#34;</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/slurm-as-a-compilation-farm/">
    <span class="title">Â« Prev</span>
    <br>
    <span>SLURM as a Compilation Farm</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/hello-world/">
    <span class="title">Next Â»</span>
    <br>
    <span>Hello World</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on x"
            href="https://x.com/intent/tweet/?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f&amp;title=SLURM%20as%20a%20Compilation%20Farm&amp;summary=SLURM%20as%20a%20Compilation%20Farm&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f&title=SLURM%20as%20a%20Compilation%20Farm">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on whatsapp"
            href="https://api.whatsapp.com/send?text=SLURM%20as%20a%20Compilation%20Farm%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on telegram"
            href="https://telegram.me/share/url?text=SLURM%20as%20a%20Compilation%20Farm&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share SLURM as a Compilation Farm on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=SLURM%20as%20a%20Compilation%20Farm&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fslurm-as-a-compilation-farm-old%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>Â© Thamjeed</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
